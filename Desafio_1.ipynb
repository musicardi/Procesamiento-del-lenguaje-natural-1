{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq6j8LsYq1Dr"
      },
      "source": [
        "### **Consigna del desafío 1**\n",
        "\n",
        "----\n",
        "\n",
        "### Vectorización de texto y modelo de clasificación Naïve Bayes con el dataset 20 newsgroups\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJgf6GQIIEH1"
      },
      "source": [
        "**Cada experimento realizado debe estar acompañado de una explicación o interpretación de lo observado.**\n",
        "\n",
        "**1**. Vectorizar documentos. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n",
        "Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido\n",
        "la similaridad según el contenido del texto y la etiqueta de clasificación."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# PUNTO 1 - Vectorización y análisis de similitud entre documentos\n",
        "# =================================================================\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ---------- Cargar dataset de sklearn ----------\n",
        "newsgroups_train = fetch_20newsgroups(\n",
        "    subset='train',\n",
        "    categories=None,  # Carga todas las 20 categorías\n",
        "    remove=('headers', 'footers', 'quotes')\n",
        ")\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"text\": newsgroups_train.data,\n",
        "    \"label\": [newsgroups_train.target_names[i] for i in newsgroups_train.target]\n",
        "})\n",
        "\n",
        "print(f\"Cantidad de documentos: {len(df)}\")\n",
        "print(f\"Categorías: {df['label'].unique()}\\n\")\n",
        "\n",
        "# ---------- Vectorización con TF-IDF ----------\n",
        "# Se mantiene ngram_range=(1,2) como estaba originalmente en tu código de P1\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), max_df=0.95, min_df=2, stop_words='english')\n",
        "X = tfidf.fit_transform(df['text'])\n",
        "\n",
        "# ---------- Seleccionar 5 documentos aleatorios ----------\n",
        "random.seed(42)\n",
        "selected_idxs = random.sample(range(df.shape[0]), 5)\n",
        "\n",
        "# ---------- Calcular similitud coseno y mostrar top-5 ----------\n",
        "for idx in selected_idxs:\n",
        "    vec = X[idx]\n",
        "    sims = cosine_similarity(vec, X).flatten()\n",
        "    top_idxs = sims.argsort()[::-1][:6]  # incluye el mismo documento\n",
        "    top_idxs = [i for i in top_idxs if i != idx][:5]\n",
        "\n",
        "    print(\"=\"*100)\n",
        "    print(f\"Documento {idx} (Etiqueta: {df.loc[idx,'label']})\")\n",
        "    print(f\"Texto:\\n{df.loc[idx,'text'][:400]}...\\n\")\n",
        "\n",
        "    same_label = 0\n",
        "    for rank, j in enumerate(top_idxs, start=1):\n",
        "        match_label = df.loc[j, 'label']\n",
        "        if match_label == df.loc[idx, 'label']:\n",
        "            same_label += 1\n",
        "        print(f\"  {rank}. Similitud={sims[j]:.3f} | Etiqueta={match_label}\")\n",
        "        print(f\"     → {df.loc[j,'text'][:200].replace('\\n',' ')}...\\n\")\n",
        "\n",
        "    print(f\"→ Coinciden {same_label} de 5 en la misma categoría.\\n\")\n",
        "\n",
        "print(\"\\nAnálisis general (con 20 categorías):\")\n",
        "print(\"Al usar más categorías, es probable que la coincidencia de etiquetas en el top-5\")\n",
        "print(\"sea ligeramente menor que con solo 5 categorías debido a la mayor diversidad temática,\")\n",
        "print(\"pero la similitud coseno debería seguir agrupando bien documentos relacionados.\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLMOELDPsfSe",
        "outputId": "21d42019-fb23-46af-a299-74663ef81a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de documentos: 11314\n",
            "Categorías: ['rec.autos' 'comp.sys.mac.hardware' 'comp.graphics' 'sci.space'\n",
            " 'talk.politics.guns' 'sci.med' 'comp.sys.ibm.pc.hardware'\n",
            " 'comp.os.ms-windows.misc' 'rec.motorcycles' 'talk.religion.misc'\n",
            " 'misc.forsale' 'alt.atheism' 'sci.electronics' 'comp.windows.x'\n",
            " 'rec.sport.hockey' 'rec.sport.baseball' 'soc.religion.christian'\n",
            " 'talk.politics.mideast' 'talk.politics.misc' 'sci.crypt']\n",
            "\n",
            "====================================================================================================\n",
            "Documento 10476 (Etiqueta: rec.sport.hockey)\n",
            "Texto:\n",
            "This is a general question for US readers:\n",
            "\n",
            "How extensive is the playoff coverage down there?  In Canada, it is almost\n",
            "impossible not to watch a series on TV (ie the only two series I have not had\n",
            "an opportunity to watch this year are Wash-NYI and Chi-Stl, the latter because\n",
            "I'm in the wrong time zone!).  We (in Canada) are basically swamped with \n",
            "coverage, and I wonder how many series/games are t...\n",
            "\n",
            "  1. Similitud=0.139 | Etiqueta=rec.sport.baseball\n",
            "     → I hate to be the burden of bad news, but I think I will this time!  =)  The Phillies usually play at either 7:05 P.M. or 7:35 P.M. Eastern Time for weekdays.  On Sundays the time is usually 1:35 P.M. ...\n",
            "\n",
            "  2. Similitud=0.128 | Etiqueta=rec.sport.hockey\n",
            "     →  I only have one comment on this:  You call this a *classic* playoff year and yet you don't include a Chicago-Detroit series.  C'mon, I'm a Boston fan and I even realize that Chicago-Detroit games are...\n",
            "\n",
            "  3. Similitud=0.123 | Etiqueta=rec.sport.baseball\n",
            "     → : \t\t\tWatch us soar in 1993!   Shouldn't that be 'Watch us stoned in 1993!'? :)  or maybe 'Watch us suck in 1993!'  or even 'Watch us sore in 1993!'...\n",
            "\n",
            "  4. Similitud=0.119 | Etiqueta=rec.sport.baseball\n",
            "     →    Hank Greenberg would have to be the most famous, because his Jewish faith actually affected his play. (missing late season or was it world series games because of Yom Kippur)   ...\n",
            "\n",
            "  5. Similitud=0.116 | Etiqueta=sci.crypt\n",
            "     → i don't have FTP and i live in canada ( this means that it would be  illeagle for a U.S. citizen to send the program to me. their gigerment  wishes to restrict its dispersil ) but someone in europe mu...\n",
            "\n",
            "→ Coinciden 1 de 5 en la misma categoría.\n",
            "\n",
            "====================================================================================================\n",
            "Documento 1824 (Etiqueta: comp.sys.mac.hardware)\n",
            "Texto:\n",
            "\n",
            "\n",
            "\tI think this kind of comparison is pretty useless in general.  The\n",
            "processor is only good when a good computer is designed around it adn the\n",
            "computer is used in its designed purpose.  Comparing processor speed is\n",
            "pretty dumb because all you have to do is just increase the clock speed\n",
            "to increase speed among other things.\n",
            "\n",
            "\tI mean how can you say a 040 is faster than a 486 without \n",
            "giving is ope...\n",
            "\n",
            "  1. Similitud=0.295 | Etiqueta=comp.sys.mac.hardware\n",
            "     → dhk@ubbpc.uucp (Dave Kitabjian) writes ...  040 486 030 386 020 286   060 fastest, then Pentium, with the first versions of the PowerPC somewhere in the vicinity.   No.  Computer speed is only partly ...\n",
            "\n",
            "  2. Similitud=0.289 | Etiqueta=comp.sys.mac.hardware\n",
            "     → rvenkate@ux4.cso.uiuc.edu (Ravikuma Venkateswar) writes ...  Benchmarks are for marketing dweebs and CPU envy.  OK, if it will make you happy, the 486 is faster than the 040.  BFD.  Both architectures...\n",
            "\n",
            "  3. Similitud=0.214 | Etiqueta=comp.sys.mac.hardware\n",
            "     → Well folks, after some thought the answer struck me flat in the face:  \"Why would Apple release a Duo Dock with a processor of its own?\"  Here's why- People have hounded Apple for a notebook with a 68...\n",
            "\n",
            "  4. Similitud=0.175 | Etiqueta=comp.sys.mac.hardware\n",
            "     → I'm sure Intel and Motorola are competing neck-and-neck for  crunch-power, but for a given clock speed, how do we rank the following (from 1st to 6th):   486\t\t68040   386\t\t68030   286\t\t68020  While yo...\n",
            "\n",
            "  5. Similitud=0.172 | Etiqueta=comp.sys.mac.hardware\n",
            "     → If you get the Centris 650 with CD configuration, you are getting a Mac with a 68RC040 processor that has built-in math coprocessor support.  My  understanding is that the \"optional fpu\" refers to you...\n",
            "\n",
            "→ Coinciden 5 de 5 en la misma categoría.\n",
            "\n",
            "====================================================================================================\n",
            "Documento 409 (Etiqueta: comp.graphics)\n",
            "Texto:\n",
            "I can't fiqure this out.  I have properly compiled pov on a unix machine\n",
            "running SunOS 4.1.3  The problem is that when I run the sample .pov files and\n",
            "use the EXACT same parameters when compiling different .tga outputs.  Some\n",
            "of the .tga's are okay, and other's are unrecognizable by any software....\n",
            "\n",
            "  1. Similitud=0.209 | Etiqueta=comp.graphics\n",
            "     → Hi, I'm just getting into PoVRay and I was wondering if there is a graphic package that outputs .POV files.  Any help would be appreciated. Thanks.  Later'ish Craig ...\n",
            "\n",
            "  2. Similitud=0.171 | Etiqueta=comp.graphics\n",
            "     → hi guys  like all people in this group i'm a fans of fractal and render sw  my favourite are fractint pov & 3dstudio 2.0   now listen my ideas  i'have just starting now to be able to use 3dstudio quit...\n",
            "\n",
            "  3. Similitud=0.163 | Etiqueta=comp.graphics\n",
            "     →  Hallo POV-Renderers ! I've got a BocaX3 Card. Now I try to get POV displaying True Colors while rendering. I've tried most of the options and UNIVESA-Driver but what happens isn't correct. Can anybod...\n",
            "\n",
            "  4. Similitud=0.129 | Etiqueta=comp.windows.x\n",
            "     →    Or use a SunOS 4.1.1 ld....\n",
            "\n",
            "  5. Similitud=0.121 | Etiqueta=comp.graphics\n",
            "     → I finally got a 24 bit viewer for my POVRAY generated .TGA files.  It was written in C by Sean Malloy and he kindly sent me a copy.  He wrote it for the same purpose, to view .TGA files using his Spee...\n",
            "\n",
            "→ Coinciden 4 de 5 en la misma categoría.\n",
            "\n",
            "====================================================================================================\n",
            "Documento 4506 (Etiqueta: rec.autos)\n",
            "Texto:\n",
            "\n",
            "This does sound good, but I heard it tends to leave more grit, etc in the \n",
            "oil pan.  Also, I've been told to change the old when it's hot before the\n",
            "grit has much time to settle.\n",
            "\n",
            "Any opinions?\n",
            "...\n",
            "\n",
            "  1. Similitud=0.136 | Etiqueta=rec.motorcycles\n",
            "     →   It's normal for the BMW K bikes to use a little oil in the first few thousand  miles.  I don't know why.  I've had three new K bikes, and all three used a bit of oil when new - max maybe .4 quart in...\n",
            "\n",
            "  2. Similitud=0.127 | Etiqueta=rec.autos\n",
            "     →  [Long silly discussion deleted...]   This suggestion isn't as far-fetched as it sounds.  Years ago in another time and place, I used to do oil changes in boats powered by automotive engines.  In many...\n",
            "\n",
            "  3. Similitud=0.107 | Etiqueta=rec.autos\n",
            "     → My friend brought a subaru SVX recently.  I had drove it for couples times and I think its a great car, esp on snow.  However when she took it to a local Subaru dealer for a oil change, the bill came ...\n",
            "\n",
            "  4. Similitud=0.088 | Etiqueta=rec.autos\n",
            "     → Why crawl under the car at all? I have a machine I got for my boat that  pulls the oil out under suction through the dip stick tube. It does an excellent job and by moving the suction tube around, you...\n",
            "\n",
            "  5. Similitud=0.088 | Etiqueta=comp.os.ms-windows.misc\n",
            "     →     I've been told that Panasonic has uploaded some to Compu$erve, but I don't have a CIS account.  I just use the Epson FX-80 driver myself, and it comes out very pretty (if very slowly) on my 1080i....\n",
            "\n",
            "→ Coinciden 3 de 5 en la misma categoría.\n",
            "\n",
            "====================================================================================================\n",
            "Documento 4012 (Etiqueta: rec.sport.hockey)\n",
            "Texto:\n",
            "For those Leaf fans who are concerned, the following players are slated for\n",
            "return on Thursday's Winnipeg-Toronto game :\n",
            "    Peter Zezel, John Cullen\n",
            "\n",
            "  Mark Osborne and Dave Ellett are questionable to return on Thursday....\n",
            "\n",
            "  1. Similitud=0.193 | Etiqueta=rec.sport.hockey\n",
            "     → In  <1qvos8$r78@cl.msu.>, vergolin@euler.lbs.msu.edu (David Vergolini) writes...  There's quite a few Wings fans lurking about here, they just tend to be low key and thoughtful rather than woofers.  I...\n",
            "\n",
            "  2. Similitud=0.117 | Etiqueta=rec.sport.hockey\n",
            "     →   Implicitly you are assuming that goals scored against Winnipeg with Selanne on the ice can be blamed on him...Roger, he is a FORWARD.  Winnipeg has a lousy defensive record anyway.  Let's put it ano...\n",
            "\n",
            "  3. Similitud=0.097 | Etiqueta=rec.sport.hockey\n",
            "     → Toronto                          1 1 1--3 Detroit                          1 4 1--6 First period      1, Detroit, Yzerman 1 (Gallant, Ciccarelli) 4:48.      2, Toronto, Cullen 1 (Clark, Gill) 10:44. S...\n",
            "\n",
            "  4. Similitud=0.096 | Etiqueta=rec.sport.hockey\n",
            "     → Detroit is a very disciplined team.  There's a lot of Europeans in Detroit which would make the game fast, so Toronto would have to slow the game down, which means drawing penalties, as a last resort ...\n",
            "\n",
            "  5. Similitud=0.096 | Etiqueta=rec.sport.hockey\n",
            "     →  IMO any good player should score on power plays because of the man advantage.  Very good power play scorers tend to become overrated because their point totals are inflated by power play points. +/- ...\n",
            "\n",
            "→ Coinciden 5 de 5 en la misma categoría.\n",
            "\n",
            "\n",
            "Análisis general (con 20 categorías):\n",
            "Al usar más categorías, es probable que la coincidencia de etiquetas en el top-5\n",
            "sea ligeramente menor que con solo 5 categorías debido a la mayor diversidad temática,\n",
            "pero la similitud coseno debería seguir agrupando bien documentos relacionados.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2**. Construir un modelo de clasificación por prototipos (tipo zero-shot). Clasificar los documentos de un conjunto de test comparando cada uno con todos los de entrenamiento y asignar la clase al label del documento del conjunto de entrenamiento con mayor similaridad."
      ],
      "metadata": {
        "id": "BPiEcpuQtRWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# PUNTO 2 - Clasificación por prototipos (tipo zero-shot)\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ---------- Cargar dataset ----------\n",
        "newsgroups = fetch_20newsgroups(\n",
        "    subset='train',\n",
        "    categories=None,  # Carga las 20 categorías disponibles\n",
        "    remove=('headers', 'footers', 'quotes')\n",
        ")\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"text\": newsgroups.data,\n",
        "    \"label\": [newsgroups.target_names[i] for i in newsgroups.target]\n",
        "})\n",
        "\n",
        "print(f\"Cantidad de documentos: {len(df)}\")\n",
        "print(f\"Categorías (20 clases): {len(df['label'].unique())}\\n\")\n",
        "\n",
        "# ---------- Split train/test ----------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['text'],\n",
        "    df['label'],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['label']\n",
        ")\n",
        "\n",
        "# ---------- Vectorización ----------\n",
        "tfidf = TfidfVectorizer(\n",
        "    ngram_range=(1,2),\n",
        "    max_df=0.95,\n",
        "    min_df=2,\n",
        "    stop_words='english'\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "# ---------- Clasificación tipo zero-shot ----------\n",
        "# Cada documento del test se compara con TODOS los de entrenamiento,\n",
        "# se asigna la etiqueta del documento más similar.\n",
        "\n",
        "y_pred = []\n",
        "\n",
        "for i in range(X_test_tfidf.shape[0]):\n",
        "    # El cálculo de similitud coseno es la parte más costosa\n",
        "    sims = cosine_similarity(X_test_tfidf[i], X_train_tfidf).flatten()\n",
        "    best_idx = sims.argmax()\n",
        "    pred_label = y_train.iloc[best_idx]\n",
        "    y_pred.append(pred_label)\n",
        "\n",
        "# ---------- Evaluación ----------\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy del modelo por prototipos (20 clases): {acc:.3f}\\n\")\n",
        "print(\"Reporte de clasificación:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# ---------- Ejemplo de predicción ----------\n",
        "print(\"=\"*100)\n",
        "print(\"Ejemplo de comparación (primer documento del test):\\n\")\n",
        "print(\"Texto test:\\n\", X_test.iloc[0][:400].replace('\\n', ' '), \"...\\n\")\n",
        "\n",
        "sims = cosine_similarity(X_test_tfidf[0], X_train_tfidf).flatten()\n",
        "best_idx = sims.argmax()\n",
        "\n",
        "print(f\"Predicción: {y_train.iloc[best_idx]}  |  Etiqueta real: {y_test.iloc[0]}\\n\")\n",
        "print(\"Documento de entrenamiento más similar:\\n\", X_train.iloc[best_idx][:400].replace('\\n', ' '), \"...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3prdbTt9tdZE",
        "outputId": "e9139bd5-726e-4d1d-e43a-df73fd3f5864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de documentos: 11314\n",
            "Categorías (20 clases): 20\n",
            "\n",
            "\n",
            "Accuracy del modelo por prototipos (20 clases): 0.620\n",
            "\n",
            "Reporte de clasificación:\n",
            "\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.67      0.64      0.65        96\n",
            "           comp.graphics       0.52      0.54      0.53       117\n",
            " comp.os.ms-windows.misc       0.57      0.56      0.57       118\n",
            "comp.sys.ibm.pc.hardware       0.50      0.48      0.49       118\n",
            "   comp.sys.mac.hardware       0.56      0.60      0.58       115\n",
            "          comp.windows.x       0.70      0.67      0.68       119\n",
            "            misc.forsale       0.58      0.56      0.57       117\n",
            "               rec.autos       0.53      0.48      0.50       119\n",
            "         rec.motorcycles       0.65      0.60      0.63       120\n",
            "      rec.sport.baseball       0.65      0.69      0.67       119\n",
            "        rec.sport.hockey       0.71      0.72      0.72       120\n",
            "               sci.crypt       0.76      0.73      0.74       119\n",
            "         sci.electronics       0.63      0.60      0.62       118\n",
            "                 sci.med       0.47      0.73      0.57       119\n",
            "               sci.space       0.70      0.64      0.67       119\n",
            "  soc.religion.christian       0.65      0.66      0.66       120\n",
            "      talk.politics.guns       0.72      0.70      0.71       109\n",
            "   talk.politics.mideast       0.73      0.73      0.73       113\n",
            "      talk.politics.misc       0.66      0.61      0.63        93\n",
            "      talk.religion.misc       0.46      0.35      0.40        75\n",
            "\n",
            "                accuracy                           0.62      2263\n",
            "               macro avg       0.62      0.61      0.62      2263\n",
            "            weighted avg       0.62      0.62      0.62      2263\n",
            "\n",
            "====================================================================================================\n",
            "Ejemplo de comparación (primer documento del test):\n",
            "\n",
            "Texto test:\n",
            " Has anyone found a fix for the following problem?  Client Software:\tSunOs 4.1.1, X11R5 Server Hardware:\tSun IPC Server Software:\tSunOs 4.1.1, Open Windows 3.0 (w/ patch 100444-37)  A Motif 1.2.2 application will periodically hang when run against the OpenWindows 3.0 server (xnews). The pulldown is displayed but then no button actions have any effect. Sometimes pressing <Return> will unstick the ap ...\n",
            "\n",
            "Predicción: comp.windows.x  |  Etiqueta real: comp.windows.x\n",
            "\n",
            "Documento de entrenamiento más similar:\n",
            " :  : Has anyone found a fix for the following problem? :  : Client Software:\tSunOs 4.1.1, X11R5 : Server Hardware:\tSun IPC : Server Software:\tSunOs 4.1.1, Open Windows 3.0 (w/ patch 100444-37) :  : A Motif 1.2.2 application will periodically hang when run against the : OpenWindows 3.0 server (xnews). The pulldown is displayed but then no : button actions have any effect. Sometimes pressing <Return ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3**. Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación\n",
        "(f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros\n",
        "de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial\n",
        "y ComplementNB.\n",
        "\n",
        "**NO cambiar el hiperparámetro ngram_range de los vectorizadores**."
      ],
      "metadata": {
        "id": "Kexd2U9wtdoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------\n",
        "# PUNTO 3 - Modelos de clasificación Naïve Bayes (Optimizado con GridSearchCV)\n",
        "# ----------------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# 1. Definición de X e Y\n",
        "X_data = df['text']\n",
        "y_data = df['label']\n",
        "\n",
        "# 2. División train / test\n",
        "# Aseguramos que la división se haga ANTES de la vectorización si usamos Pipeline\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=42)\n",
        "\n",
        "# Definición de la grilla de hiperparámetros a buscar\n",
        "# RESTRICCIÓN: ngram_range debe ser (1, 1).\n",
        "param_grid = {\n",
        "    # Parámetros del Vectorizador (CountVectorizer)\n",
        "    'vectorizer__stop_words': ['english'],\n",
        "    'vectorizer__ngram_range': [(1, 1)],    # Se mantiene fijo según la instrucción\n",
        "    # Parámetros del Modelo Naïve Bayes\n",
        "    'model__alpha': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0] # Tuning para maximizar el desempeño\n",
        "}\n",
        "\n",
        "# --- Modelo 1: Naive Bayes Multinomial (Optimización) ---\n",
        "print(\"=====================================================\")\n",
        "print(\"=== 1. Naïve Bayes Multinomial (Optimización) ===\")\n",
        "print(\"=====================================================\")\n",
        "\n",
        "# Crear el Pipeline\n",
        "pipeline_multi = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('model', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Configurar GridSearchCV\n",
        "grid_search_multi = GridSearchCV(\n",
        "    pipeline_multi,\n",
        "    param_grid,\n",
        "    scoring='f1_macro', # Métrica objetivo\n",
        "    cv=3,              # Cross-validation\n",
        "    n_jobs=-1          # Usar todos los núcleos\n",
        ")\n",
        "\n",
        "# Entrenar la búsqueda de grilla\n",
        "grid_search_multi.fit(X_train, y_train)\n",
        "\n",
        "# Evaluación en el conjunto de test con el mejor modelo\n",
        "y_pred_multi_best = grid_search_multi.predict(X_test)\n",
        "f1_multi_best = f1_score(y_test, y_pred_multi_best, average='macro')\n",
        "\n",
        "print(\"\\nResultados MultinomialNB:\")\n",
        "print(f\"Mejor alpha (optimizado): {grid_search_multi.best_params_['model__alpha']}\")\n",
        "print(f\"F1-macro en Test (Mejor modelo): {round(f1_multi_best, 3)}\")\n",
        "print(classification_report(y_test, y_pred_multi_best))\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- Modelo 2: Naive Bayes ComplementNB (Optimización) ---\n",
        "print(\"\\n=====================================================\")\n",
        "print(\"=== 2. Naïve Bayes ComplementNB (Optimización) ===\")\n",
        "print(\"=====================================================\")\n",
        "\n",
        "# Crear el Pipeline\n",
        "pipeline_comp = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('model', ComplementNB())\n",
        "])\n",
        "\n",
        "# Configurar GridSearchCV\n",
        "grid_search_comp = GridSearchCV(\n",
        "    pipeline_comp,\n",
        "    param_grid, # Misma grilla de búsqueda (solo cambia el modelo)\n",
        "    scoring='f1_macro',\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Entrenar la búsqueda de grilla\n",
        "grid_search_comp.fit(X_train, y_train)\n",
        "\n",
        "# Evaluación en el conjunto de test con el mejor modelo\n",
        "y_pred_comp_best = grid_search_comp.predict(X_test)\n",
        "f1_comp_best = f1_score(y_test, y_pred_comp_best, average='macro')\n",
        "\n",
        "print(\"\\nResultados ComplementNB:\")\n",
        "print(f\"Mejor alpha (optimizado): {grid_search_comp.best_params_['model__alpha']}\")\n",
        "print(f\"F1-macro en Test (Mejor modelo): {round(f1_comp_best, 3)}\")\n",
        "print(classification_report(y_test, y_pred_comp_best))\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- Comparación final ---\n",
        "print(\"\\n Comparación Final (F1-macro en Test)\")\n",
        "print(f\"MultinomialNB (Mejor alpha): {round(f1_multi_best, 3)}\")\n",
        "print(f\"ComplementNB (Mejor alpha): {round(f1_comp_best, 3)}\")\n",
        "\n",
        "# Determinación del mejor modelo\n",
        "if f1_comp_best > f1_multi_best:\n",
        "    mejor_modelo = \"ComplementNB\"\n",
        "    mejor_f1 = f1_comp_best\n",
        "elif f1_multi_best > f1_comp_best:\n",
        "    mejor_modelo = \"MultinomialNB\"\n",
        "    mejor_f1 = f1_multi_best\n",
        "else:\n",
        "    mejor_modelo = \"Ambos modelos tienen el mismo F1-macro en Test\"\n",
        "    mejor_f1 = f1_multi_best\n",
        "\n",
        "print(\"\\n Mejor modelo para maximizar el desempeño (F1-macro en Test):\", mejor_modelo)\n",
        "print(\"F1-macro máximo alcanzado:\", round(mejor_f1, 3))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10SKIogXtfy_",
        "outputId": "764d037f-3d4b-4f04-dd6c-5f1775f95d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================================================\n",
            "=== 1. Naïve Bayes Multinomial (Optimización) ===\n",
            "=====================================================\n",
            "\n",
            "Resultados MultinomialNB:\n",
            "Mejor alpha (optimizado): 0.1\n",
            "F1-macro en Test (Mejor modelo): 0.694\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.67      0.72      0.70       135\n",
            "           comp.graphics       0.57      0.67      0.61       166\n",
            " comp.os.ms-windows.misc       1.00      0.04      0.07       170\n",
            "comp.sys.ibm.pc.hardware       0.56      0.80      0.66       182\n",
            "   comp.sys.mac.hardware       0.75      0.77      0.76       183\n",
            "          comp.windows.x       0.71      0.82      0.76       169\n",
            "            misc.forsale       0.79      0.70      0.74       172\n",
            "               rec.autos       0.79      0.75      0.77       191\n",
            "         rec.motorcycles       0.79      0.79      0.79       198\n",
            "      rec.sport.baseball       0.86      0.83      0.84       168\n",
            "        rec.sport.hockey       0.58      0.80      0.68       163\n",
            "               sci.crypt       0.80      0.75      0.78       195\n",
            "         sci.electronics       0.77      0.66      0.71       177\n",
            "                 sci.med       0.79      0.83      0.81       172\n",
            "               sci.space       0.83      0.79      0.81       176\n",
            "  soc.religion.christian       0.67      0.86      0.75       182\n",
            "      talk.politics.guns       0.79      0.76      0.78       173\n",
            "   talk.politics.mideast       0.75      0.78      0.77       160\n",
            "      talk.politics.misc       0.61      0.71      0.66       156\n",
            "      talk.religion.misc       0.58      0.36      0.44       107\n",
            "\n",
            "                accuracy                           0.72      3395\n",
            "               macro avg       0.73      0.71      0.69      3395\n",
            "            weighted avg       0.74      0.72      0.70      3395\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "=====================================================\n",
            "=== 2. Naïve Bayes ComplementNB (Optimización) ===\n",
            "=====================================================\n",
            "\n",
            "Resultados ComplementNB:\n",
            "Mejor alpha (optimizado): 0.5\n",
            "F1-macro en Test (Mejor modelo): 0.704\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.47      0.66      0.55       135\n",
            "           comp.graphics       0.68      0.69      0.68       166\n",
            " comp.os.ms-windows.misc       0.81      0.18      0.29       170\n",
            "comp.sys.ibm.pc.hardware       0.64      0.75      0.69       182\n",
            "   comp.sys.mac.hardware       0.89      0.75      0.81       183\n",
            "          comp.windows.x       0.62      0.89      0.73       169\n",
            "            misc.forsale       0.72      0.66      0.69       172\n",
            "               rec.autos       0.87      0.73      0.79       191\n",
            "         rec.motorcycles       0.89      0.79      0.84       198\n",
            "      rec.sport.baseball       0.89      0.83      0.86       168\n",
            "        rec.sport.hockey       0.71      0.87      0.78       163\n",
            "               sci.crypt       0.74      0.83      0.78       195\n",
            "         sci.electronics       0.77      0.62      0.69       177\n",
            "                 sci.med       0.80      0.86      0.83       172\n",
            "               sci.space       0.82      0.82      0.82       176\n",
            "  soc.religion.christian       0.63      0.87      0.73       182\n",
            "      talk.politics.guns       0.77      0.76      0.77       173\n",
            "   talk.politics.mideast       0.59      0.87      0.70       160\n",
            "      talk.politics.misc       0.80      0.67      0.73       156\n",
            "      talk.religion.misc       0.68      0.21      0.33       107\n",
            "\n",
            "                accuracy                           0.73      3395\n",
            "               macro avg       0.74      0.71      0.70      3395\n",
            "            weighted avg       0.75      0.73      0.72      3395\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            " Comparación Final (F1-macro en Test)\n",
            "MultinomialNB (Mejor alpha): 0.694\n",
            "ComplementNB (Mejor alpha): 0.704\n",
            "\n",
            " Mejor modelo para maximizar el desempeño (F1-macro en Test): ComplementNB\n",
            "F1-macro máximo alcanzado: 0.704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4**. Transponer la matriz documento-término. De esa manera se obtiene una matriz\n",
        "término-documento que puede ser interpretada como una colección de vectorización de palabras.\n",
        "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares.\n",
        "\n",
        "**Elegir las palabras MANUALMENTE para evitar la aparición de términos poco interpretables**."
      ],
      "metadata": {
        "id": "Vb-izhYKtgCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# PUNTO 4 - Similaridad entre palabras (matriz término-documento)\n",
        "# ================================================================\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- 1. Cargar el dataset completo (20 categorías) ----------\n",
        "newsgroups_train = fetch_20newsgroups(\n",
        "    subset='train',\n",
        "    categories=None,  # Carga todas las 20 categorías\n",
        "    remove=('headers', 'footers', 'quotes')\n",
        ")\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"text\": newsgroups_train.data\n",
        "})\n",
        "print(f\"Documentos cargados para el Punto 4: {len(df)}\")\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "# Vectorización unigramas: ngram_range=(1, 1) y min_df=5 para reducir el vocabulario\n",
        "# (Se añade min_df=5 como optimización para reducir ligeramente la carga, aunque no es el problema principal)\n",
        "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1,1), min_df=5)\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "\n",
        "# Matriz término-documento (traspuesta)\n",
        "X_t = X.T\n",
        "vocab = np.array(vectorizer.get_feature_names_out())\n",
        "print(f\"Tamaño del Vocabulario después de filtrado: {len(vocab)}\")\n",
        "\n",
        "# Elegir 5 palabras clave.\n",
        "selected_words = ['windows', 'space', 'baseball', 'encryption', 'cancer']\n",
        "selected_indices = []\n",
        "\n",
        "for word in selected_words:\n",
        "    if word in vocab:\n",
        "        selected_indices.append(np.where(vocab == word)[0][0])\n",
        "    else:\n",
        "        print(f\" La palabra '{word}' no se encuentra en el vocabulario.\")\n",
        "\n",
        "# CÁLCULO DE SIMILITUD OPTIMIZADO:\n",
        "# Solo calculamos la similitud de las 5 palabras seleccionadas contra TODAS las demás.\n",
        "# Esto reduce el cálculo de (100k x 100k) a (5 x 100k).\n",
        "X_selected = X_t[selected_indices]\n",
        "similarity_matrix_optimized = cosine_similarity(X_selected, X_t)\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(\"\\n--- Análisis de Similaridad entre Palabras (20 Clases) ---\\n\")\n",
        "\n",
        "for i, word in enumerate(selected_words):\n",
        "    if word in vocab:\n",
        "        # Fila de similitudes para la palabra actual\n",
        "        sims = similarity_matrix_optimized[i]\n",
        "\n",
        "        # Obtiene los 6 mejores índices y omite el primer (la palabra misma)\n",
        "        top_indices = sims.argsort()[::-1][1:6]\n",
        "        similar_words = vocab[top_indices]\n",
        "        print(f\" Palabra: '{word}'\")\n",
        "        print(\"    Más similares:\", ', '.join(similar_words))\n",
        "    # No se necesita else aquí porque el filtro ya se hizo antes\n",
        "\n",
        "print(\"\\n-------------------------------------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIWqUs6fthJc",
        "outputId": "ba8733a7-4fc9-4bbf-f3b4-6c278f0dfe8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documentos cargados para el Punto 4: 11314\n",
            "Tamaño del Vocabulario después de filtrado: 17797\n",
            "\n",
            "--- Análisis de Similaridad entre Palabras (20 Clases) ---\n",
            "\n",
            " Palabra: 'windows'\n",
            "    Más similares: hardware, nt, microsoft, software, using\n",
            " Palabra: 'space'\n",
            "    Más similares: aerospace, satellites, exploration, nasa, satellite\n",
            " Palabra: 'baseball'\n",
            "    Más similares: hispanic, clemens, pitching, ripken, stats\n",
            " Palabra: 'encryption'\n",
            "    Más similares: accommodates, torrance, heyman, pitted, nistnews\n",
            " Palabra: 'cancer'\n",
            "    Más similares: asthma, centers, particles, avenue, airborne\n",
            "\n",
            "-------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CcjOJhDiHDr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones Generales del Trabajo Final\n",
        "\n",
        "El proyecto se basó en el análisis y clasificación del *dataset* **20 Newsgroups** en su totalidad (**20 categorías temáticas**), comparando enfoques de procesamiento de lenguaje natural (NLP) basados en **Similitud por Distribución**, **Clasificación Zero-Shot** y **Modelos Supervisados Clásicos** (Naïve Bayes optimizado).\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Similitud Documental y Clasificación Zero-Shot\n",
        "\n",
        "### **Similitud Documental (Punto 1)**\n",
        "\n",
        "La combinación de la representación **TF-IDF** con la **Similitud Coseno** demostró ser efectiva para agrupar documentos por tema, incluso con 20 categorías.\n",
        "\n",
        "| Documento Base (Categoría) | Fidelidad (Coincidentes / 5) | Observación |\n",
        "| :--- | :--- | :--- |\n",
        "| **`comp.sys.mac.hardware`** | **5 de 5** | Alta coherencia; temas muy técnicos. |\n",
        "| **`rec.sport.hockey`** | **5 de 5** | Coherencia perfecta; vocabulario especializado. |\n",
        "| **`rec.autos`** | **3 de 5** | Solapamiento con temas de `rec.motorcycles`, mostrando la mayor ambigüedad temática. |\n",
        "\n",
        "* **Conclusión:** La métrica de similitud coseno sobre los vectores TF-IDF captura la **coherencia temática** del corpus. La caída en los valores máximos de similitud (respecto a 5 clases) es esperada por el aumento del vocabulario, pero el agrupamiento por etiqueta se mantiene sólido en temas bien definidos.\n",
        "\n",
        "### **Clasificación por Prototipos (Punto 2)**\n",
        "\n",
        "* **Resultado Clave:** Se obtuvo una **Accuracy del $\\mathbf{0.620}$** y un **F1-macro del $\\mathbf{0.62}$** en el conjunto de prueba.\n",
        "* **Interpretación:** Para un problema de **20 clases**, un F1-macro de $0.62$ es aceptable para un modelo *zero-shot* basado en el vecino más cercano. La **caída de rendimiento** (respecto al $\\approx 0.82$ de 5 clases) confirma la mayor dificultad del problema con más etiquetas.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Modelos de Clasificación Naïve Bayes (Punto 3)\n",
        "\n",
        "### **Optimización y Comparación (Máximo F1-macro)**\n",
        "\n",
        "Se optimizó el hiperparámetro de suavizado ($\\alpha$) usando *Grid Search* sobre un **CountVectorizer** con **unigramas** (`ngram_range=(1, 1)`).\n",
        "\n",
        "| Modelo | F1-macro en Test (Final) | Mejor $\\alpha$ |\n",
        "| :--- | :--- | :--- |\n",
        "| **Naïve Bayes Multinomial** | $\\mathbf{0.694}$ | $0.1$ |\n",
        "| **Naïve Bayes ComplementNB** | $\\mathbf{0.704}$ | $0.5$ |\n",
        "\n",
        "**Conclusión Final del Punto 3:**\n",
        "El modelo **ComplementNB** fue el que alcanzó el máximo desempeño, superando a MultinomialNB con un **F1-macro de $\\mathbf{0.704}$**. Esta robustez se debe a su mejor manejo de las clases con menor representación o con *features* menos distintivas. Se observa que la clasificación en 20 clases presenta serios desafíos en categorías como `comp.os.ms-windows.misc` (F1 $\\approx 0.07$ en MNB) y `talk.religion.misc` (F1 $\\approx 0.33$ en CNB), donde el *recall* es notablemente bajo.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Similaridad entre Palabras (Punto 4)\n",
        "\n",
        "#### **Análisis de Coocurrencia Contextual (Matriz Término $\\times$ Documento)**\n",
        "\n",
        "La transposición de la matriz término-documento... permitió analizar la **afinidad contextual** entre los términos.\n",
        "\n",
        "* **Coherencia Fuerte:** Palabras como **'space'** y **'baseball'** mostraron relaciones temáticas claras y esperadas..., lo que valida que la coocurrencia modelada captura la estructura semántica en categorías bien definidas.\n",
        "* **Fallo Contextual (Coherencia Nula):** Términos como **'encryption'** y **'cancer'** generaron listas de vecinos completamente **incoherentes** ('torrance', 'pitted', 'airborne'). Esto indica que su vector de coocurrencia es dominado por el **ruido**, impidiendo una interpretación semántica clara.\n",
        "\n",
        "**Conclusión Final del Punto 4:**\n",
        "El análisis valida que la similitud coseno sobre la matriz transpuesta captura las **relaciones de coocurrencia implícitas** solo para términos con alta distinción temática. El fracaso en palabras como 'encryption' sugiere una limitación en este método para términos con baja frecuencia o amplio solapamiento inter-clase, **un detalle crucial para la interpretación de los resultados.**\n",
        "\n",
        "---\n",
        "\n",
        "## Resumen General del Proyecto\n",
        "\n",
        "1.  **Representación:** **TF-IDF** demostró ser una representación vectorial robusta y efectiva para la medición de similaridad y clasificación inicial en 20 clases.\n",
        "2.  **Clasificación:** El modelo supervisado **ComplementNB** optimizado se consolidó como la **solución de mejor desempeño** al alcanzar un F1-macro de **$\\mathbf{0.704}$**, superando al MultinomialNB.\n",
        "3.  **Análisis:** El estudio de la matriz término-documento confirmó que las relaciones semánticas son un reflejo directo y fiel del contexto temático **solo para los términos con alta distinción temática** (ej. 'space', 'baseball'). Se observaron **fallos en la coherencia** para términos con baja frecuencia o amplio solapamiento inter-clase (ej. 'encryption', 'cancer'), lo que señala una **limitación clave** de la representación para el *dataset* completo."
      ],
      "metadata": {
        "id": "uS6SuHE6u0ab"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}