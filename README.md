
Este repositorio contiene los desaf√≠os y proyectos completados durante el curso de Procesamiento de Lenguaje Natural (PLN). Los trabajos abarcan desde t√©cnicas fundamentales de clasificaci√≥n y vectorizaci√≥n, 
hasta la implementaci√≥n de modelos avanzados de lenguaje y traducci√≥n con arquitecturas recurrentes (RNN/LSTM).

## üë®‚Äçüè´ Equipo del Curso

| Rol | Nombre | Periodo |
| :--- | :--- | :--- |
| **Alumno** | Lic. Juan Manuel Calabia | 
| **Profesor** | Dr. Rodrigo Cardenas Szigety | 
| **Profesor** | Dr. Nicol√°s Vattuone | 
| **Profesor** | Esp. Ing. Hern√°n Contigiani | 


# üåü Resumen General de la Materia: Procesamiento de Lenguaje Natural (PLN)

Esta materia ofrece una visi√≥n integral de las t√©cnicas y modelos utilizados para que las m√°quinas procesen y entiendan el lenguaje humano. El curso se divide en tres pilares fundamentales:

## 1. Fundamentos y Representaci√≥n del Lenguaje

Se establecen las bases del PLN, ense√±ando a la computadora a "leer":
* **Vectorizaci√≥n:** T√©cnicas para convertir texto en n√∫meros (ej. TF-IDF).
* **Embeddings:** Construcci√≥n de representaciones vectoriales sem√°nticas (**Word2Vec, CBOW, Skip-Gram**) que capturan el significado de las palabras.
* **Aplicaci√≥n:** Uso de estas herramientas en la construcci√≥n de **Information Retrieval Bots** (Clases 1-3).

## 2. Modelos de Secuencia Profunda (*Deep Learning*)

Se cubren las arquitecturas neuronales dise√±adas para procesar secuencias de texto:
* **Redes Recurrentes (RNN/LSTM):** Estudio de modelos para manejar dependencias en el tiempo, aplicados a la predicci√≥n de la pr√≥xima palabra y el **An√°lisis de Sentimiento** (Clases 4-5).
* **Sequence-to-Sequence (Seq2Seq):** Arquitectura Encoder-Decoder, pilar de los **Traductores Autom√°ticos** y los **Bots Conversacionales** (Clase 6).

## 3. PLN de Vanguardia y Producci√≥n

Se exploran los modelos m√°s avanzados y se ense√±a a llevarlos al mundo real:
* **Attention y Transformers:** Introducci√≥n al mecanismo de **Attention** y el modelo **Transformer** (la base de los LLMs modernos).
* **Modelos Pre-entrenados:** Uso de modelos avanzados como **BERT** y **ELMo** mediante **Fine Tuning** (Clase 7).
* ***Deployment*** **Industrial:** Cierre pr√°ctico con el **Deployment de Servicios NLP** usando herramientas como **Flask**, **Docker** y **Tensorflow Serving (TFX)** (Clase 8).

  
# üöÄ Resumen de Proyectos

## Desaf√≠o 1: Clasificaci√≥n de Textos con Na√Øve Bayes

### Vectorizaci√≥n de texto y Clasificaci√≥n Na√Øve Bayes

* **Objetivo:** Clasificar documentos del dataset **20 newsgroups** despu√©s de convertirlos a una representaci√≥n vectorial.
* **Metodolog√≠a Clave:**
    * **Vectorizaci√≥n:** Uso de **TF-IDF** para la representaci√≥n num√©rica de documentos.
    * **Clasificaci√≥n:** Implementaci√≥n y optimizaci√≥n de modelos **Multinomial Na√Øve Bayes** y **Complement Na√Øve Bayes**.
* **Resultado Destacado:**
    * El modelo **ComplementNB** demostr√≥ ser la **soluci√≥n de mejor desempe√±o**, alcanzando un **F1-macro de 0.704**.

---

## Desaf√≠o 2: Custom Embeddings con Word2Vec

### Creaci√≥n de Embeddings Personalizados con Gensim

* **Objetivo:** Generar representaciones vectoriales (*embeddings*) de palabras basadas en un contexto tem√°tico espec√≠fico.
* **Corpus Utilizado:** Letras de canciones de **Jimi Hendrix**, para capturar el lenguaje l√≠rico y simb√≥lico del artista.
* **Metodolog√≠a Clave:**
    * Entrenamiento de un modelo **Word2Vec** con arquitectura **Skip-Gram** sobre el corpus de letras.
    * An√°lisis de similitud entre palabras clave (ej. 'love', 'fire', 'power').
    * Visualizaci√≥n del espacio vectorial mediante **t-SNE**.
* **Conclusi√≥n:**
    * El modelo captur√≥ la esencia sem√°ntica, revelando n√∫cleos tem√°ticos claros: emocional, vital y conceptual (libertad/poder). El espacio vectorial refleja una constelaci√≥n de **amor, energ√≠a y conciencia**.

---

## Desaf√≠o 3: Modelo de Lenguaje a Nivel de Car√°cter (RNN)

### Implementaci√≥n de un Modelo de Lenguaje Recurrente

* **Objetivo:** Construir un modelo de lenguaje con redes neuronales recurrentes (**RNN**) que opere a nivel de car√°cter para generar nuevas secuencias de texto.
* **Metodolog√≠a Clave:**
    * **Tokenizaci√≥n por caracteres** del corpus seleccionado.
    * Implementaci√≥n de la arquitectura recurrente para predecir el siguiente car√°cter.
    * Exploraci√≥n de t√©cnicas de decodificaci√≥n para la generaci√≥n de texto.
* **Estrategias de Generaci√≥n Exploradas:**
    * **Greedy Search**
    * **Beam Search** (determin√≠stico y estoc√°stico, analizando el impacto de la temperatura en la diversidad de las salidas).

---

## Desaf√≠o 4: Traductor Secuencia a Secuencia (LSTM Seq2Seq)

### Traducci√≥n Autom√°tica Neuronal con LSTM Encoder-Decoder

* **Objetivo:** Construir un sistema de Traducci√≥n Autom√°tica Neuronal (NMT) utilizando la arquitectura fundamental **Sequence-to-Sequence (Seq2Seq)** con redes **LSTM**.
* **Metodolog√≠a Clave:**
    * Uso de un dataset de traducciones de Anki.
    * Dise√±o del modelo **Encoder-Decoder** en una *framework* de *deep learning*.
* **Estrategias de Decodificaci√≥n y Muestreo Exploradas:**
    * **Greedy Search**
    * **Sampling**
    * **Top-K Sampling**
    * **Top-P (Nucleus) Sampling**
    * **Beam Search**
* **Foco:** El trabajo se centr√≥ en la comparaci√≥n de estas estrategias para mejorar tanto la calidad como la diversidad de las traducciones generadas.
