{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfa39F4lsLf3"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## LSTM Traductor\n",
        "Ejemplo basado en [LINK](https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqO0PRcFsPTe"
      },
      "source": [
        "### Datos\n",
        "El objecto es utilizar datos disponibles de Anki de traducciones de texto en diferentes idiomas. Se construirá un modelo traductor seq2seq utilizando encoder-decoder.\\ [LINK](https://www.manythings.org/anki/)\n",
        "\n",
        "\n",
        "Replicar y extender el traductor:- Replicar el modelo en PyTorch.- Extender el entrenamiento a más datos y tamaños de secuencias mayores.- Explorar el impacto de la cantidad de neuronas en las capas recurrentes.- Mostrar 5 ejemplos de traducciones generadas.- Extras que se pueden probar: Embeddings pre-entrenados para los dos idiomas; cambiar la estrategia de generación (por ejemplo muestreo aleatorio);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8mUC7SzYlV8"
      },
      "source": [
        "# Traductor Seq2Seq con Atención Bahdanau\n",
        "\n",
        "## Nota Importante\n",
        "\n",
        "El siguiente código implementa un modelo Seq2Seq con mecanismo de atención Bahdanau. El modelo fue entrenado con solo 5 épocas debido a las limitaciones de tiempo y recursos de GPU, lo cual es insuficiente para obtener traducciones de alta calidad en este dataset.\n",
        "\n",
        "## Descripción\n",
        "\n",
        "Este proyecto implementa un traductor neuronal Seq2Seq (Sequence-to-Sequence) con LSTM y mecanismo de atención Bahdanau que traduce del inglés al español. La atención permite al modelo enfocarse en diferentes partes de la secuencia de entrada al generar cada palabra de salida.\n",
        "\n",
        "## Arquitectura del Modelo\n",
        "\n",
        "### Componentes Principales\n",
        "\n",
        "- **Encoder (LSTM)**\n",
        "  - Embedding dimension: 256\n",
        "  - Hidden dimension: 512\n",
        "  - Procesa la secuencia de entrada completa\n",
        "  - Genera representaciones contextuales para cada token\n",
        "\n",
        "- **Decoder (LSTM) con Atención Bahdanau**\n",
        "  - Embedding dimension: 256\n",
        "  - Hidden dimension: 512\n",
        "  - Mecanismo de atención que calcula pesos dinámicos sobre las salidas del encoder\n",
        "  - Concatena el contexto atencional con el embedding de entrada\n",
        "  - Genera predicciones token por token\n",
        "\n",
        "- **Mecanismo de Atención Bahdanau**\n",
        "  - Calcula scores de alineamiento entre el estado oculto del decoder y todas las salidas del encoder\n",
        "  - Utiliza una red feed-forward con activación tanh\n",
        "  - Produce pesos de atención normalizados con softmax\n",
        "  - Genera un vector de contexto ponderado\n",
        "\n",
        "### Fórmulas de Atención\n",
        "```\n",
        "score = V * tanh(W1 * encoder_outputs + W2 * decoder_hidden)\n",
        "attention_weights = softmax(score)\n",
        "context = sum(attention_weights * encoder_outputs)\n",
        "```\n",
        "\n",
        "## Configuración del Entrenamiento\n",
        "\n",
        "- **Dataset**: spa-eng (TensorFlow) - ~118,000 pares de oraciones\n",
        "- **Épocas completadas**: 5 (de 20 planificadas)\n",
        "- **Batch size**: 128\n",
        "- **Máxima longitud de secuencia**: 40 tokens\n",
        "- **Learning rate**: 0.001\n",
        "- **Optimizador**: Adam\n",
        "- **Loss**: CrossEntropyLoss (ignorando padding)\n",
        "- **Teacher forcing ratio**: 0.5\n",
        "- **Device**: CUDA (GPU) / CPU compatible\n",
        "\n",
        "## Ventajas de la Atención Bahdanau\n",
        "\n",
        "1. **Alineamiento Dinámico**: El modelo aprende automáticamente a alinear palabras entre idiomas\n",
        "2. **Manejo de Secuencias Largas**: Supera el problema del \"cuello de botella\" del Seq2Seq básico\n",
        "3. **Interpretabilidad**: Los pesos de atención muestran qué partes de la entrada son relevantes\n",
        "4. **Mejor Rendimiento**: Generalmente produce traducciones más precisas que Seq2Seq sin atención\n",
        "\n",
        "## Limitaciones Actuales\n",
        "\n",
        "### Entrenamiento Insuficiente\n",
        "- Solo 5 épocas completadas (objetivo: 20-50 épocas)\n",
        "- Las traducciones pueden ser inconsistentes o de baja calidad\n",
        "- El modelo aún no ha convergido completamente\n",
        "\n",
        "### Recursos Computacionales\n",
        "- Entrenamiento interrumpido por límites de tiempo de GPU\n",
        "- Cada época requiere aproximadamente 15-20 minutos con GPU\n",
        "- Total estimado para convergencia: 5-8 horas\n",
        "\n",
        "## Comparación con Seq2Seq Básico\n",
        "\n",
        "| Característica | Seq2Seq Básico | Seq2Seq + Atención |\n",
        "|---------------|----------------|-------------------|\n",
        "| Contexto | Vector fijo | Dinámico por paso |\n",
        "| Secuencias largas | Limitado | Mejor manejo |\n",
        "| Interpretabilidad | Baja | Alta (visualizar atención) |\n",
        "| Parámetros | Menos | Más |\n",
        "| Rendimiento | Bueno | Mejor |\n",
        "\n",
        "## Requisitos Técnicos\n",
        "\n",
        "- Python 3.7+\n",
        "- PyTorch 1.8+\n",
        "- Bibliotecas: numpy, json, zipfile, gdown\n",
        "\n",
        "\n",
        "# Problemas de Entrenamiento en Colab Free\n",
        "\n",
        "## Restricciones de Hardware\n",
        "\n",
        "Colab Free a veces proporciona una GPU \"real\", pero con las siguientes limitaciones:\n",
        "\n",
        "* **T4:** Se obtiene la mayoría del tiempo. Está **capada** (limitada en potencia, con throttling).\n",
        "* **P100:** Se obtiene a veces. También con **throttling**.\n",
        "* **L4:** Se obtiene muy cada tanto. Dura **poco tiempo**.\n",
        "* **A100/Serias:** Nunca se obtienen en el modo gratuito.\n",
        "\n",
        "## Restricciones por Duración de Uso\n",
        "\n",
        "La potencia de la GPU se reduce progresivamente en función del tiempo de uso:\n",
        "\n",
        "1.  **5–10 minutos iniciales:** La GPU opera a \"full power\".\n",
        "2.  **Después de $\\approx 10$ minutos (Uso Intensivo):** Se detecta el uso y se **baja la frecuencia** de la GPU.\n",
        "3.  **Más de 1 hora de entrenamiento:** Se **baja aún más** la potencia.\n",
        "4.  **2 horas o más:**\n",
        "    * Se **apaga la sesión**.\n",
        "    * O se deja funcionando **solo con CPU**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF6sQkV-I8vR",
        "outputId": "2c6f8099-1793-4def-f6d6-c2519d499a63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Device: cpu\n",
            "Total de pares cargados: 118964\n",
            "Vocab EN: 23851\n",
            "Vocab ES: 41723\n",
            "Cargando modelo completo guardado...\n",
            "\n",
            "EN: tom invited mary to dinner.\n",
            "ES: tom le a mary a mary.\n",
            "\n",
            "EN: we love you.\n",
            "ES: nos te\n",
            "\n",
            "EN: i didn't get the point of his speech.\n",
            "ES: no me la la de su\n",
            "\n",
            "EN: i have a stomachache.\n",
            "ES: tengo un un\n",
            "\n",
            "EN: you guys need new shoes.\n",
            "ES: te ves a\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# SEQ2SEQ + ATENCIÓN (Bahdanau)\n",
        "# ==============================\n",
        "\n",
        "# 0) IMPORTS / MOUNT DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import gdown\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import tempfile\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.modules.sparse import Embedding\n",
        "from torch.nn.modules.rnn import LSTM, GRU\n",
        "from torch.nn.modules.linear import Linear\n",
        "from torch.nn.modules.dropout import Dropout\n",
        "from torch.nn.modules.activation import ReLU, Tanh, Sigmoid\n",
        "from torch.nn.modules.container import ModuleList\n",
        "\n",
        "# 1) CONFIGURACIÓN RUTAS Y DEVICE\n",
        "BASE_DIR = \"/content/drive/MyDrive/seq2seq_bahdanau\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "ZIP_FILE = \"spa-eng.zip\"\n",
        "URL = \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "DATA_DIR = \"spa-eng\"\n",
        "SPA_FILE = os.path.join(DATA_DIR, \"spa.txt\")\n",
        "\n",
        "CHECK_BATCH = os.path.join(BASE_DIR, \"checkpoint_batch.pth\")\n",
        "CHECK_EPOCH = os.path.join(BASE_DIR, \"checkpoint_epoch.pth\")\n",
        "LAST_MODEL = os.path.join(BASE_DIR, \"model_last.pt\")\n",
        "VOCAB_FILE = os.path.join(BASE_DIR, \"vocabularios.json\")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# Parámetros\n",
        "BATCH_SIZE = 128\n",
        "MAX_LEN = 40\n",
        "SAVE_EVERY_N_BATCHES = 20\n",
        "TOTAL_EPOCHS = 20\n",
        "\n",
        "# 2) DESCARGA Y DESCOMPRESION DATASET SI HACE FALTA\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    if not os.path.exists(ZIP_FILE):\n",
        "        print(\"Descargando dataset spa-eng...\")\n",
        "        gdown.download(URL, ZIP_FILE, quiet=False)\n",
        "    print(\"Descomprimiendo spa-eng.zip...\")\n",
        "    with zipfile.ZipFile(ZIP_FILE, \"r\") as z:\n",
        "        z.extractall(\".\")\n",
        "\n",
        "# 3) LECTURA DEL DATASET\n",
        "sentences_en = []\n",
        "sentences_es = []\n",
        "\n",
        "with open(SPA_FILE, encoding=\"utf-8\") as f:\n",
        "    lines = f.read().strip().split(\"\\n\")\n",
        "\n",
        "for line in lines:\n",
        "    parts = line.split(\"\\t\")\n",
        "    if len(parts) >= 2:\n",
        "        en, es = parts[0], parts[1]\n",
        "        sentences_en.append(en.lower())\n",
        "        sentences_es.append(es.lower())\n",
        "\n",
        "print(\"Total de pares cargados:\", len(sentences_en))\n",
        "\n",
        "# 4) VOCABULARIOS Y ENCODING\n",
        "SOS, EOS, PAD = \"<sos>\", \"<eos>\", \"<pad>\"\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    vocab = {PAD:0, SOS:1, EOS:2}\n",
        "    idx = 3\n",
        "    for s in sentences:\n",
        "        for w in s.split():\n",
        "            if w not in vocab:\n",
        "                vocab[w] = idx\n",
        "                idx += 1\n",
        "    inv = {i:w for w,i in vocab.items()}\n",
        "    return vocab, inv\n",
        "\n",
        "src_vocab, inv_src = build_vocab(sentences_en)\n",
        "tgt_vocab, inv_tgt = build_vocab(sentences_es)\n",
        "\n",
        "# guardar vocabularios para reuso\n",
        "with open(VOCAB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\n",
        "        \"src_vocab\": src_vocab,\n",
        "        \"tgt_vocab\": tgt_vocab\n",
        "    }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Vocab EN:\", len(src_vocab))\n",
        "print(\"Vocab ES:\", len(tgt_vocab))\n",
        "\n",
        "def encode(sentence, vocab):\n",
        "    return [vocab[SOS]] + [vocab.get(w,0) for w in sentence.split()] + [vocab[EOS]]\n",
        "\n",
        "# 5) DATASET + DATALOADER\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, en, es):\n",
        "        self.en = en\n",
        "        self.es = es\n",
        "    def __len__(self):\n",
        "        return len(self.en)\n",
        "    def __getitem__(self, idx):\n",
        "        src = encode(self.en[idx], src_vocab)\n",
        "        tgt = encode(self.es[idx], tgt_vocab)\n",
        "        src = src[:MAX_LEN] + [0]*(MAX_LEN - len(src))\n",
        "        tgt = tgt[:MAX_LEN] + [0]*(MAX_LEN - len(tgt))\n",
        "        return torch.tensor(src, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)\n",
        "\n",
        "ds = TranslationDataset(sentences_en, sentences_es)\n",
        "train_loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# 6) MODELO (Bahdanau LSTM)\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hid_dim):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(hid_dim, hid_dim)\n",
        "        self.W2 = nn.Linear(hid_dim, hid_dim)\n",
        "        self.V  = nn.Linear(hid_dim, 1)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        hidden_time = hidden[-1].unsqueeze(1)  # (B,1,H)\n",
        "        score = torch.tanh(self.W1(encoder_outputs) + self.W2(hidden_time))\n",
        "        score = self.V(score).squeeze(-1)      # (B,T)\n",
        "        attn = torch.softmax(score, dim=1)     # (B,T)\n",
        "        context = torch.bmm(attn.unsqueeze(1), encoder_outputs) # (B,1,H)\n",
        "        return context.squeeze(1), attn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab, emb=256, hid=512):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab, emb, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(emb, hid, batch_first=True)\n",
        "    def forward(self, src):\n",
        "        emb = self.embedding(src)\n",
        "        out, hidden = self.lstm(emb)\n",
        "        return out, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab, emb=256, hid=512):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab, emb, padding_idx=0)\n",
        "        self.attn = BahdanauAttention(hid)\n",
        "        self.lstm = nn.LSTM(emb + hid, hid, batch_first=True)\n",
        "        self.fc = nn.Linear(hid, vocab)\n",
        "    def forward(self, token, hidden, encoder_outputs):\n",
        "        emb = self.embedding(token).unsqueeze(1)\n",
        "        context, _ = self.attn(hidden[0], encoder_outputs)\n",
        "        context = context.unsqueeze(1)\n",
        "        lstm_in = torch.cat([emb, context], dim=-1)\n",
        "        out, hidden = self.lstm(lstm_in, hidden)\n",
        "        pred = self.fc(out.squeeze(1))\n",
        "        return pred, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, enc, dec):\n",
        "        super().__init__()\n",
        "        self.enc = enc\n",
        "        self.dec = dec\n",
        "    def forward(self, src, tgt, teacher=0.5):\n",
        "        enc_out, hidden = self.enc(src)\n",
        "        B,T = tgt.shape\n",
        "        outputs = torch.zeros(B, T, self.dec.fc.out_features).to(DEVICE)\n",
        "        token = tgt[:,0]\n",
        "        for t in range(1,T):\n",
        "            out, hidden = self.dec(token, hidden, enc_out)\n",
        "            outputs[:,t] = out\n",
        "            token = tgt[:,t] if random.random()<teacher else out.argmax(1)\n",
        "        return outputs\n",
        "\n",
        "# 7) GUARDADO (Drive-friendly)\n",
        "def _atomic_save(obj, dst_path):\n",
        "    \"\"\"Guardar directamente en dst_path con torch.save (Drive-friendly).\"\"\"\n",
        "    try:\n",
        "        torch.save(obj, dst_path)\n",
        "        size = os.path.getsize(dst_path)\n",
        "        print(f\"Guardado OK -> {dst_path} ({size} bytes)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"Error guardando directamente en Drive:\", e)\n",
        "        return False\n",
        "\n",
        "def save_batch_checkpoint(path, epoch, batch_idx, model, opt):\n",
        "    payload = {\n",
        "        \"epoch\": epoch,\n",
        "        \"batch\": batch_idx,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"opt_state\": opt.state_dict(),\n",
        "        \"timestamp\": time.time()\n",
        "    }\n",
        "    _atomic_save(payload, path)\n",
        "\n",
        "def save_epoch_checkpoint(path, epoch, model, opt):\n",
        "    payload = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"opt_state\": opt.state_dict(),\n",
        "        \"timestamp\": time.time()\n",
        "    }\n",
        "    _atomic_save(payload, path)\n",
        "\n",
        "def save_model_last(path, model):\n",
        "    payload = {\"model_state\": model.state_dict(), \"timestamp\": time.time()}\n",
        "    _atomic_save(payload, path)\n",
        "\n",
        "def load_checkpoint_safe(path, model, opt=None):\n",
        "    \"\"\"\n",
        "    Intenta cargar checkpoint; si está corrupto o no existe, devuelve (1,0)\n",
        "    Si existe, retorna (start_epoch, start_batch) y carga estados.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        return 1, 0\n",
        "    try:\n",
        "        ck = torch.load(path, map_location=DEVICE)\n",
        "        if \"model_state\" in ck:\n",
        "            model.load_state_dict(ck[\"model_state\"])\n",
        "        if opt is not None and \"opt_state\" in ck and ck[\"opt_state\"] is not None:\n",
        "            opt.load_state_dict(ck[\"opt_state\"])\n",
        "        start_epoch = ck.get(\"epoch\", 1)\n",
        "        start_batch = ck.get(\"batch\", 0)\n",
        "        return start_epoch, start_batch\n",
        "    except Exception as e:\n",
        "        print(\"No pude cargar checkpoint (posible corrupción). Detalle:\", e)\n",
        "        return 1, 0\n",
        "\n",
        "# 8) FUNCIÓN DE ENTRENAMIENTO RESILIENTE\n",
        "def train_resiliente(total_epochs=TOTAL_EPOCHS):\n",
        "    enc = Encoder(len(src_vocab)).to(DEVICE)\n",
        "    dec = Decoder(len(tgt_vocab)).to(DEVICE)\n",
        "    model = Seq2Seq(enc, dec).to(DEVICE)\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    crit = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    # intentar cargar checkpoint por batch\n",
        "    start_epoch, start_batch = 1, 0\n",
        "    if os.path.exists(CHECK_BATCH):\n",
        "        print(\"Cargando checkpoint por batch (prioritario)...\")\n",
        "        se, sb = load_checkpoint_safe(CHECK_BATCH, model, opt)\n",
        "        # reanudar en el siguiente batch\n",
        "        start_epoch, start_batch = se, sb + 1\n",
        "        print(f\"Reanudando desde epoch {start_epoch}, batch {start_batch}\")\n",
        "    elif os.path.exists(CHECK_EPOCH):\n",
        "        print(\"Cargando checkpoint por epoch...\")\n",
        "        se, sb = load_checkpoint_safe(CHECK_EPOCH, model, opt)\n",
        "        # si checkpoint de epoch representa final de epoch N, reanudar en N+1\n",
        "        start_epoch, start_batch = se + 1, 0\n",
        "        print(f\"Reanudando desde epoch {start_epoch}\")\n",
        "\n",
        "    # Entrenamiento\n",
        "    for epoch in range(start_epoch, total_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for batch_idx, (src, tgt) in enumerate(train_loader):\n",
        "            # si estamos reanudando, saltamos batches ya procesados\n",
        "            if epoch == start_epoch and batch_idx < start_batch:\n",
        "                continue\n",
        "\n",
        "            src = src.to(DEVICE)\n",
        "            tgt = tgt.to(DEVICE)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            out = model(src, tgt)\n",
        "            loss = crit(out[:,1:].reshape(-1, out.shape[-1]), tgt[:,1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Guardar checkpoint por batch cada N batches (para no sobrecargar Drive)\n",
        "            if (batch_idx % SAVE_EVERY_N_BATCHES) == 0:\n",
        "                save_batch_checkpoint(CHECK_BATCH, epoch, batch_idx, model, opt)\n",
        "\n",
        "            # imprimir progreso\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"[E{epoch}/{total_epochs}] Batch {batch_idx}/{len(train_loader)}  loss={loss.item():.4f}\")\n",
        "\n",
        "        # fin epoch: guardar checkpoint por epoch y modelo \"last\"\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"✔ Epoch {epoch} completada | loss promedio: {avg_loss:.4f}\")\n",
        "\n",
        "        # Guardados a DRIVE\n",
        "        save_epoch_checkpoint(CHECK_EPOCH, epoch, model, opt)\n",
        "        save_model_last(LAST_MODEL, model)\n",
        "\n",
        "        # reset start_batch para próximas epochs\n",
        "        start_batch = 0\n",
        "\n",
        "    # al terminar, eliminar checkpoint por batch opcionalmente (ya quedó model_last)\n",
        "    if os.path.exists(CHECK_BATCH):\n",
        "        try:\n",
        "            os.remove(CHECK_BATCH)\n",
        "            print(\"Removed CHECK_BATCH (opcional).\")\n",
        "        except Exception as e:\n",
        "            print(\"No pude borrar CHECK_BATCH:\", e)\n",
        "    print(\"Entrenamiento finalizado. Modelo guardado en:\", LAST_MODEL)\n",
        "    return model\n",
        "\n",
        "# 9) FUNCIONES DE INFERENCE\n",
        "torch.serialization.add_safe_globals([\n",
        "    Seq2Seq, Encoder, Decoder, BahdanauAttention\n",
        "])\n",
        "\n",
        "# Clases internas de PyTorch usadas en tu modelo:\n",
        "torch.serialization.add_safe_globals([\n",
        "    nn.Module, nn.Sequential,\n",
        "    Embedding, Linear, Dropout,\n",
        "    GRU, LSTM,\n",
        "    ReLU, Tanh, Sigmoid,\n",
        "    ModuleList\n",
        "])\n",
        "\n",
        "def load_model_for_inference(path=LAST_MODEL):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(\"No existe un modelo guardado. Entrená primero.\")\n",
        "\n",
        "    # Permitimos explícitamente las clases que están en el checkpoint\n",
        "    torch.serialization.add_safe_globals([Seq2Seq, Encoder, Decoder, BahdanauAttention])\n",
        "\n",
        "    model_trained = torch.load(path, map_location=DEVICE)\n",
        "    model_trained.eval()\n",
        "    return model_trained\n",
        "\n",
        "def generate(model, sentence):\n",
        "    model.eval()\n",
        "    src = encode(sentence, src_vocab)\n",
        "    src = src[:MAX_LEN] + [0]*(MAX_LEN-len(src))\n",
        "    src = torch.tensor([src]).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        enc_out, hidden = model.enc(src)\n",
        "        token = torch.tensor([tgt_vocab[SOS]]).to(DEVICE)\n",
        "        result = []\n",
        "        for _ in range(MAX_LEN):\n",
        "            out, hidden = model.dec(token, hidden, enc_out)\n",
        "            nxt = out.argmax(1).item()\n",
        "            if nxt == tgt_vocab[EOS]:\n",
        "                break\n",
        "            result.append(inv_tgt.get(nxt, \"<unk>\"))\n",
        "            token = torch.tensor([nxt]).to(DEVICE)\n",
        "    return \" \".join(result)\n",
        "\n",
        "# 10) CARGAR MODELO\n",
        "print(\"Cargando modelo completo guardado...\")\n",
        "model_trained = load_model_for_inference()\n",
        "\n",
        "# Probar algunas traducciones\n",
        "for i in range(5):\n",
        "    s = random.choice(sentences_en)\n",
        "    print(\"\\nEN:\", s)\n",
        "    print(\"ES:\", generate(model_trained, s))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR6IL-PUaE0D"
      },
      "source": [
        "# Traductor Seq2Seq con Múltiples Estrategias de Decodificación\n",
        "\n",
        "## Nota Importante\n",
        "\n",
        "El siguiente código es una versión más simple que el anterior, pero el modelo fue entrenado con solo 10 épocas debido a las limitaciones de GPU, lo cual es insuficiente para este dataset. Las traducciones pueden estar vacías o ser de baja calidad.\n",
        "\n",
        "## Descripción\n",
        "\n",
        "Este proyecto implementa un traductor neuronal Seq2Seq (Sequence-to-Sequence) con LSTM que traduce del inglés al español, incluyendo 5 estrategias diferentes de decodificación.\n",
        "\n",
        "## Arquitectura del Modelo\n",
        "\n",
        "- **Encoder-Decoder con LSTM**\n",
        "- Dimensión oculta: 256\n",
        "- Capas LSTM: 1\n",
        "- Dimensión embeddings: 50\n",
        "- Tamaño vocabulario: 10,000 palabras (INPUT y TARGET)\n",
        "\n",
        "## Estrategias de Decodificación Implementadas\n",
        "\n",
        "### 1. Greedy Decoding\n",
        "- Selecciona el token con mayor probabilidad en cada paso\n",
        "- Determinista (siempre produce la misma salida)\n",
        "- Rápido pero puede quedar atrapado en soluciones subóptimas\n",
        "\n",
        "### 2. Sampling con Temperatura\n",
        "- Muestreo estocástico de la distribución de probabilidad\n",
        "- La temperatura controla la aleatoriedad (T<1: conservador, T>1: más aleatorio)\n",
        "- Genera traducciones más diversas\n",
        "\n",
        "### 3. Top-K Sampling\n",
        "- Restringe el muestreo a los k tokens más probables\n",
        "- k=50: considera solo las 50 palabras más probables\n",
        "- Balance entre diversidad y calidad\n",
        "\n",
        "### 4. Top-P (Nucleus) Sampling\n",
        "- Muestreo dinámico basado en probabilidad acumulada\n",
        "- p=0.9: selecciona el conjunto más pequeño de tokens cuya probabilidad suma ≥ 0.9\n",
        "- Adapta el tamaño del vocabulario según el contexto\n",
        "\n",
        "### 5. Beam Search\n",
        "- Mantiene las top-k hipótesis en cada paso (beam_width=5)\n",
        "- Explora múltiples secuencias en paralelo\n",
        "- Generalmente produce mejores resultados pero es más costoso computacionalmente\n",
        "\n",
        "## Configuración del Entrenamiento\n",
        "\n",
        "- **Dataset**: spa-eng (TensorFlow)\n",
        "- **Épocas**: 10\n",
        "- **Batch size**: 64\n",
        "- **Learning rate**: 1e-4\n",
        "- **Optimizador**: Adam\n",
        "- **Loss**: CrossEntropyLoss (ignorando padding)\n",
        "\n",
        "## Limitaciones y Trabajo Futuro\n",
        "\n",
        "El modelo presenta las siguientes limitaciones:\n",
        "\n",
        "- Requiere más épocas de entrenamiento (recomendado: 30-50 épocas)\n",
        "- Vocabulario limitado causa problemas con palabras poco frecuentes\n",
        "- Las traducciones pueden estar vacías debido al entrenamiento insuficiente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaC5pFutZ54E",
        "outputId": "8fe4ee42-2ede-41ad-a464-67b04c4b6087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "Usando dispositivo: cpu\n",
            "Montando Google Drive...\n",
            "Mounted at /content/drive\n",
            "Ruta de guardado configurada en: /content/drive/MyDrive/Modelos_Traductor/traductor_seq2seq.pt\n",
            "Descargando datos desde: http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2577it [00:00, 51242.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Máxima longitud de secuencia de INPUT (EN): 47\n",
            "Máxima longitud de secuencia de TARGET (ES): 51\n",
            "--------------------------------------------------\n",
            "Modelo Seq2Seq con HIDDEN_DIM=256\n",
            "--------------------------------------------------\n",
            "\n",
            "--- CARGANDO MODELO ENTRENADO ---\n",
            "¡Modelo cargado con éxito desde Google Drive! Saltando entrenamiento.\n",
            "\n",
            "==================================================\n",
            "Generación de Ejemplos con Diferentes Estrategias de Decodificación\n",
            "==================================================\n",
            "\n",
            "[1] INPUT (EN): Tom is here.\n",
            "    -> Greedy:           \n",
            "    -> Sample (T=0.8):   \n",
            "    -> Top-k (k=50):     \n",
            "    -> Top-p (p=0.9):    \n",
            "    -> Beam Search (w=5): \n",
            "\n",
            "[2] INPUT (EN): Go home.\n",
            "    -> Greedy:           \n",
            "    -> Sample (T=0.8):   Mire bastante\n",
            "    -> Top-k (k=50):     Eso\n",
            "    -> Top-p (p=0.9):    Tom bien.\n",
            "    -> Beam Search (w=5): \n",
            "\n",
            "[3] INPUT (EN): I am Tom.\n",
            "    -> Greedy:           \n",
            "    -> Sample (T=0.8):   \n",
            "    -> Top-k (k=50):     es se\n",
            "    -> Top-p (p=0.9):    \n",
            "    -> Beam Search (w=5): \n",
            "\n",
            "[4] INPUT (EN): Come here.\n",
            "    -> Greedy:           \n",
            "    -> Sample (T=0.8):   \n",
            "    -> Top-k (k=50):     fue mismo. una\n",
            "    -> Top-p (p=0.9):    Estamos\n",
            "    -> Beam Search (w=5): \n",
            "\n",
            "[5] INPUT (EN): Be quiet.\n",
            "    -> Greedy:           \n",
            "    -> Sample (T=0.8):   Que\n",
            "    -> Top-k (k=50):     \n",
            "    -> Top-p (p=0.9):    No es\n",
            "    -> Beam Search (w=5): \n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "import io\n",
        "import zipfile\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from torchinfo import summary\n",
        "from pathlib import Path\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 1. MONTAJE DE GOOGLE DRIVE Y SETUP\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# Detección del dispositivo (prioriza GPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "# Se añade una bandera para saber si la ruta de Drive es válida.\n",
        "is_colab_env = False\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    # Montar Google Drive.\n",
        "    print(\"Montando Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Definir la ruta donde se guardará el modelo\n",
        "    SAVE_DIR = '/content/drive/MyDrive/Modelos_Traductor'\n",
        "    Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "    SAVE_PATH = os.path.join(SAVE_DIR, 'traductor_seq2seq.pt')\n",
        "\n",
        "    is_colab_env = True\n",
        "    print(f\"Ruta de guardado configurada en: {SAVE_PATH}\")\n",
        "except ImportError:\n",
        "    # Si no es Colab, guarda localmente\n",
        "    SAVE_PATH = 'traductor_seq2seq.pt'\n",
        "    print(f\"No se detectó Colab. El modelo se guardará localmente en: {SAVE_PATH}\")\n",
        "\n",
        "# --- Hiperparámetros ajustados ---\n",
        "HIDDEN_DIM = 256\n",
        "NUM_LAYERS = 1\n",
        "EMBEDDING_DIM = 50\n",
        "MAX_VOCAB_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 2. DESCARGA Y PREPROCESAMIENTO DE DATOS\n",
        "# ----------------------------------------------------\n",
        "\n",
        "DATA_URL = \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "DATA_PATH = \"spa-eng.zip\"\n",
        "EXTRACT_PATH = \"spa-eng\"\n",
        "\n",
        "def download_and_load_data(url, data_path, extract_path):\n",
        "    \"\"\"Descarga, extrae y lee el dataset.\"\"\"\n",
        "    if not os.path.exists(data_path):\n",
        "        print(f\"Descargando datos desde: {url}\")\n",
        "        r = requests.get(url, stream=True)\n",
        "        with open(data_path, 'wb') as f:\n",
        "            for chunk in tqdm(r.iter_content(chunk_size=1024)):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "\n",
        "    if not os.path.exists(EXTRACT_PATH):\n",
        "        with zipfile.ZipFile(data_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "\n",
        "    file_path = os.path.join(extract_path, 'spa-eng', 'spa.txt')\n",
        "    with io.open(file_path, encoding='UTF-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    return text\n",
        "\n",
        "def preprocess_and_tokenize(text, max_vocab_size):\n",
        "    lines = text.strip().split('\\n')\n",
        "\n",
        "    input_sentences = []\n",
        "    target_sentences = []\n",
        "\n",
        "    # Itera a través de las líneas y procesa únicamente las entradas válidas\n",
        "    for line in lines:\n",
        "        # Omite líneas vacías o líneas que constan únicamente de espacios en blanco\n",
        "        if not line.strip():\n",
        "            continue\n",
        "\n",
        "        # Divide la línea por el carácter de tabulación\n",
        "        parts = line.split('\\t')\n",
        "\n",
        "        # Comprueba si la línea tiene las dos partes esperadas (origen y destino)\n",
        "        if len(parts) >= 2:\n",
        "            input_sentences.append(parts[0].strip())\n",
        "            # partes[1] es la oración de destino (p. ej., inglés)\n",
        "            target_sentences.append(parts[1].strip())\n",
        "        else:\n",
        "            # OPCIONAL: Imprime la línea problemática\n",
        "            print(f\"Skipping malformed line: '{line}'\")\n",
        "            continue\n",
        "\n",
        "    # Continua con el resto de la lógica de tokenización...\n",
        "    target_sentences = [f'<sos> {s} <eos>' for s in target_sentences]\n",
        "\n",
        "    def create_vocab(sentences, max_vocab_size):\n",
        "        word_counts = {}\n",
        "        for sentence in sentences:\n",
        "            for word in sentence.split():\n",
        "                word_counts[word] = word_counts.get(word, 0) + 1\n",
        "\n",
        "        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "        vocab = {word: i + 2 for i, (word, count) in enumerate(sorted_words) if i < max_vocab_size - 2}\n",
        "\n",
        "        vocab['<pad>'] = 0\n",
        "        vocab['<unk>'] = 1\n",
        "        idx_to_word = {v: k for k, v in vocab.items()}\n",
        "\n",
        "        return vocab, idx_to_word\n",
        "\n",
        "    word2idx_input, idx2word_input = create_vocab(input_sentences, max_vocab_size)\n",
        "    word2idx_target, idx2word_target = create_vocab(target_sentences, max_vocab_size)\n",
        "\n",
        "    def tokenize_sequences(sentences, word2idx):\n",
        "        sequences = []\n",
        "        for sentence in sentences:\n",
        "            seq = [word2idx.get(word, word2idx['<unk>']) for word in sentence.split()]\n",
        "            sequences.append(torch.tensor(seq, dtype=torch.int64))\n",
        "\n",
        "        padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=word2idx['<pad>'])\n",
        "        return padded_sequences\n",
        "\n",
        "    max_input_len = max(len(s.split()) for s in input_sentences)\n",
        "    max_target_len = max(len(s.split()) for s in target_sentences)\n",
        "\n",
        "    print(f\"Máxima longitud de secuencia de INPUT (EN): {max_input_len}\")\n",
        "    print(f\"Máxima longitud de secuencia de TARGET (ES): {max_target_len}\")\n",
        "\n",
        "    encoder_input_sequences = tokenize_sequences(input_sentences, word2idx_input)\n",
        "    decoder_input_sequences = tokenize_sequences(target_sentences, word2idx_target)\n",
        "\n",
        "    target_output_sequences = decoder_input_sequences[:, 1:]\n",
        "\n",
        "    return (encoder_input_sequences, decoder_input_sequences, target_output_sequences,\n",
        "            word2idx_input, idx2word_input, word2idx_target, idx2word_target,\n",
        "            max_input_len, max_target_len)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 3. CLASES PARA EMBEDDINGS Y MODELO\n",
        "# ----------------------------------------------------\n",
        "\n",
        "class WordsEmbeddings:\n",
        "    def __init__(self, n_features):\n",
        "        self.N_FEATURES = n_features\n",
        "    def get_words_embeddings(self, word):\n",
        "        if word in ['<pad>', '<unk>', '<sos>', '<eos>']:\n",
        "            return np.zeros((1, self.N_FEATURES))\n",
        "        # Simula un vector válido que sería reemplazado por la carga real de GloVe\n",
        "        return np.random.rand(1, self.N_FEATURES) * 0.1\n",
        "\n",
        "class GloveEmbeddings(WordsEmbeddings):\n",
        "    def __init__(self, dimension=EMBEDDING_DIM):\n",
        "        super().__init__(dimension)\n",
        "\n",
        "model_embeddings = GloveEmbeddings()\n",
        "\n",
        "def create_embedding_matrix(word2idx, num_words, embed_dim, model_embeddings):\n",
        "    \"\"\"Crea la matriz de embeddings para pasar a nn.Embedding.from_pretrained.\"\"\"\n",
        "    embedding_matrix = np.zeros((num_words, embed_dim))\n",
        "    for word, i in word2idx.items():\n",
        "        if i >= num_words:\n",
        "            continue\n",
        "        embedding_vector = model_embeddings.get_words_embeddings(word)[0]\n",
        "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_layers, lstm_size, embedding_matrix=None):\n",
        "        super().__init__()\n",
        "        self.lstm_size = lstm_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        if embedding_matrix is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(\n",
        "                torch.from_numpy(embedding_matrix).float(), freeze=True\n",
        "            )\n",
        "            embed_dim = self.embedding.embedding_dim\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(embed_dim, self.lstm_size, self.num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x.long())\n",
        "        output, hidden = self.lstm(embedded)\n",
        "        return hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embed_dim, num_layers, lstm_size, embedding_matrix=None):\n",
        "        super().__init__()\n",
        "        self.lstm_size = lstm_size\n",
        "        self.num_layers = num_layers\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        if embedding_matrix is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(\n",
        "                torch.from_numpy(embedding_matrix).float(), freeze=True\n",
        "            )\n",
        "            embed_dim = self.embedding.embedding_dim\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(output_dim, embed_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(embed_dim, self.lstm_size, self.num_layers, batch_first=True)\n",
        "        self.out = nn.Linear(self.lstm_size, output_dim)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        embedded = self.embedding(x.long())\n",
        "        output, hidden = self.lstm(embedded, hidden)\n",
        "        prediction = self.out(output.squeeze(1))\n",
        "        return prediction, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, target_len):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.target_len = target_len\n",
        "\n",
        "    def forward(self, encoder_input, decoder_input_start):\n",
        "        batch_size = decoder_input_start.shape[0]\n",
        "        vocab_size = self.decoder.output_dim\n",
        "        outputs = torch.zeros(batch_size, self.target_len, vocab_size).to(device)\n",
        "        prev_state = self.encoder(encoder_input)\n",
        "\n",
        "        for t in range(self.target_len):\n",
        "            input_t = decoder_input_start[:, t:t+1]\n",
        "            output, prev_state = self.decoder(input_t, prev_state)\n",
        "            outputs[:, t, :] = output\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class MTDataset(Dataset):\n",
        "    def __init__(self, input_data, decoder_input, target_output):\n",
        "        self.input_data = input_data\n",
        "        self.decoder_input = decoder_input\n",
        "        self.target_output = target_output\n",
        "    def __len__(self):\n",
        "        return len(self.input_data)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_data[idx], self.decoder_input[idx], self.target_output[idx]\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, clip=1):\n",
        "    \"\"\"Bucle de entrenamiento de una época.\"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, (input_seq, decoder_input_seq, target_output_seq) in enumerate(dataloader):\n",
        "        input_seq = input_seq.to(device)\n",
        "        decoder_input_seq = decoder_input_seq.to(device)\n",
        "        target_output_seq = target_output_seq.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input_seq, decoder_input_seq)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output.reshape(-1, output_dim)\n",
        "        target_output_seq = target_output_seq.reshape(-1)\n",
        "\n",
        "        loss = criterion(output, target_output_seq)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 4. INFERENCIA Y GENERACIÓN\n",
        "# ----------------------------------------------------\n",
        "\n",
        "def prepare_input_translation(sentence, word2idx, max_input_len):\n",
        "    \"\"\"Tokeniza y prepara un tensor de entrada para la traducción.\"\"\"\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
        "\n",
        "    tokens = [word2idx.get(word, word2idx['<unk>']) for word in sentence.split()]\n",
        "\n",
        "    padded_tokens = np.zeros(max_input_len, dtype=np.int32)\n",
        "    padded_tokens[-len(tokens):] = tokens\n",
        "\n",
        "    input_tensor = torch.from_numpy(padded_tokens).unsqueeze(0).to(device)\n",
        "    return input_tensor\n",
        "\n",
        "def translate_sentence(encoder_sequence_test_tensor, model, idx2word_target, word2idx_target, max_len, strategy='greedy'):\n",
        "    \"\"\"Traduce una secuencia de entrada (Greedy o Sample).\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    sos_idx = word2idx_target['<sos>']\n",
        "    eos_idx = word2idx_target['<eos>']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prev_state = model.encoder(encoder_sequence_test_tensor)\n",
        "\n",
        "    target_seq = torch.ones(1, 1).fill_(sos_idx).int().to(device)\n",
        "    translation = []\n",
        "\n",
        "    for t in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            output_logits, prev_state = model.decoder(target_seq, prev_state)\n",
        "\n",
        "        probabilities = F.softmax(output_logits.squeeze(0), dim=-1)\n",
        "\n",
        "        if strategy == 'greedy':\n",
        "            next_token_idx = probabilities.argmax().item()\n",
        "        elif strategy == 'sample':\n",
        "            next_token_idx = torch.multinomial(probabilities, num_samples=1).item()\n",
        "        else:\n",
        "            raise ValueError(\"Estrategia no reconocida. Use 'greedy' o 'sample'.\")\n",
        "\n",
        "        if next_token_idx == eos_idx:\n",
        "            break\n",
        "\n",
        "        word = idx2word_target.get(next_token_idx, '<unk>')\n",
        "        if word not in ['<pad>', '<sos>', '<eos>', '<unk>']:\n",
        "            translation.append(word)\n",
        "\n",
        "        target_seq = torch.tensor([[next_token_idx]], dtype=torch.int64).to(device)\n",
        "\n",
        "    model.train()\n",
        "    return ' '.join(translation)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 5. EJECUCIÓN PRINCIPAL Y CONFIGURACIÓN DEL MODELO\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# 1. Carga y preprocesamiento\n",
        "raw_text = download_and_load_data(DATA_URL, DATA_PATH, EXTRACT_PATH)\n",
        "(encoder_input_sequences, decoder_input_sequences, target_output_sequences,\n",
        " word2idx_input, idx2word_input, word2idx_target, idx2word_target,\n",
        " max_input_len, max_target_len) = preprocess_and_tokenize(raw_text, MAX_VOCAB_SIZE)\n",
        "\n",
        "# 2. Preparación de Embeddings\n",
        "num_words_input = len(word2idx_input)\n",
        "num_words_target = len(word2idx_target)\n",
        "\n",
        "embedding_matrix_input = create_embedding_matrix(word2idx_input, num_words_input, EMBEDDING_DIM, model_embeddings)\n",
        "embedding_matrix_output = create_embedding_matrix(word2idx_target, num_words_target, EMBEDDING_DIM, model_embeddings)\n",
        "\n",
        "# 3. Inicialización del Modelo\n",
        "encoder = Encoder(vocab_size=num_words_input, embed_dim=EMBEDDING_DIM, num_layers=NUM_LAYERS,\n",
        "                  lstm_size=HIDDEN_DIM, embedding_matrix=embedding_matrix_input)\n",
        "decoder = Decoder(output_dim=num_words_target, embed_dim=EMBEDDING_DIM, num_layers=NUM_LAYERS,\n",
        "                  lstm_size=HIDDEN_DIM, embedding_matrix=embedding_matrix_output)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, target_len=target_output_sequences.shape[1]).to(device)\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"Modelo Seq2Seq con HIDDEN_DIM={HIDDEN_DIM}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 4. DataLoaders y Optimizador\n",
        "dataset = MTDataset(encoder_input_sequences, decoder_input_sequences, target_output_sequences)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word2idx_target['<pad>']).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 6. CARGAR O ENTRENAR Y GUARDAR\n",
        "# ----------------------------------------------------\n",
        "\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    print(f\"\\n--- CARGANDO MODELO ENTRENADO ---\")\n",
        "    # Cargar el modelo asegurando que se mapee al dispositivo correcto (CPU o CUDA)\n",
        "    model.load_state_dict(torch.load(SAVE_PATH, map_location=device))\n",
        "    print(\"¡Modelo cargado con éxito desde Google Drive! Saltando entrenamiento.\")\n",
        "else:\n",
        "    print(f\"\\n--- INICIO DEL ENTRENAMIENTO ---\")\n",
        "    print(f\"Modelo no encontrado en {SAVE_PATH}. Iniciando entrenamiento desde cero.\")\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_loss = train(model, dataloader, optimizer, criterion)\n",
        "        print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f}')\n",
        "\n",
        "    # Guardar el modelo recién entrenado\n",
        "    print(f\"\\nGuardando el estado del modelo entrenado en: {SAVE_PATH}...\")\n",
        "    torch.save(model.state_dict(), SAVE_PATH)\n",
        "    print(\"¡Modelo guardado con éxito en Google Drive!\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 7. EJEMPLOS DE TRADUCCIÓN CON TODAS LAS ESTRATEGIAS\n",
        "# ----------------------------------------------------\n",
        "def translate_sentence(encoder_sequence_test_tensor, model, idx2word_target, word2idx_target, max_len, strategy='greedy', **kwargs):\n",
        "    \"\"\"\n",
        "    Traduce una secuencia de entrada usando diferentes estrategias de decodificación.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    sos_idx = word2idx_target['<sos>']\n",
        "    eos_idx = word2idx_target['<eos>']\n",
        "\n",
        "    # Beam Search (requiere lógica diferente)\n",
        "    if strategy == 'beam_search':\n",
        "        beam_width = kwargs.get('beam_width', 5)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoder_state = model.encoder(encoder_sequence_test_tensor)\n",
        "\n",
        "        beams = [(torch.ones(1, 1).fill_(sos_idx).int().to(device), 0.0, encoder_state)]\n",
        "        completed_beams = []\n",
        "\n",
        "        for t in range(max_len):\n",
        "            candidates = []\n",
        "\n",
        "            for seq, score, state in beams:\n",
        "                if seq[0, -1].item() == eos_idx:\n",
        "                    completed_beams.append((seq, score))\n",
        "                    continue\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    output_logits, new_state = model.decoder(seq[:, -1:], state)\n",
        "\n",
        "                log_probs = F.log_softmax(output_logits.squeeze(0), dim=-1)\n",
        "                top_log_probs, top_indices = torch.topk(log_probs, beam_width)\n",
        "\n",
        "                for i in range(beam_width):\n",
        "                    next_token = top_indices[i].item()\n",
        "                    next_score = score + top_log_probs[i].item()\n",
        "                    next_seq = torch.cat([seq, torch.tensor([[next_token]], dtype=torch.int64).to(device)], dim=1)\n",
        "                    candidates.append((next_seq, next_score, new_state))\n",
        "\n",
        "            if not candidates:\n",
        "                break\n",
        "\n",
        "            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "        completed_beams.extend([(seq, score) for seq, score, _ in beams])\n",
        "\n",
        "        if not completed_beams:\n",
        "            model.train()\n",
        "            return \"\"\n",
        "\n",
        "        best_seq, _ = max(completed_beams, key=lambda x: x[1])\n",
        "\n",
        "        translation = []\n",
        "        for token_idx in best_seq[0, 1:].tolist():\n",
        "            if token_idx == eos_idx:\n",
        "                break\n",
        "            word = idx2word_target.get(token_idx, '<unk>')\n",
        "            if word not in ['<pad>', '<sos>', '<eos>', '<unk>']:\n",
        "                translation.append(word)\n",
        "\n",
        "        model.train()\n",
        "        return ' '.join(translation)\n",
        "\n",
        "    # Estrategias paso a paso (greedy, sample, top_k, top_p)\n",
        "    with torch.no_grad():\n",
        "        prev_state = model.encoder(encoder_sequence_test_tensor)\n",
        "\n",
        "    target_seq = torch.ones(1, 1).fill_(sos_idx).int().to(device)\n",
        "    translation = []\n",
        "\n",
        "    for t in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            output_logits, prev_state = model.decoder(target_seq, prev_state)\n",
        "\n",
        "        probabilities = F.softmax(output_logits.squeeze(0), dim=-1)\n",
        "\n",
        "        if strategy == 'greedy':\n",
        "            next_token_idx = probabilities.argmax().item()\n",
        "\n",
        "        elif strategy == 'sample':\n",
        "            temperature = kwargs.get('temperature', 1.0)\n",
        "            if temperature != 1.0:\n",
        "                probabilities = F.softmax(output_logits.squeeze(0) / temperature, dim=-1)\n",
        "            next_token_idx = torch.multinomial(probabilities, num_samples=1).item()\n",
        "\n",
        "        elif strategy == 'top_k':\n",
        "            k = kwargs.get('k', 50)\n",
        "            top_k_probs, top_k_indices = torch.topk(probabilities, k)\n",
        "            top_k_probs = top_k_probs / top_k_probs.sum()\n",
        "            next_token_idx = top_k_indices[torch.multinomial(top_k_probs, num_samples=1)].item()\n",
        "\n",
        "        elif strategy == 'top_p':\n",
        "            p = kwargs.get('p', 0.9)\n",
        "            sorted_probs, sorted_indices = torch.sort(probabilities, descending=True)\n",
        "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "            sorted_indices_to_remove = cumulative_probs > p\n",
        "            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
        "            sorted_indices_to_remove[0] = False\n",
        "\n",
        "            filtered_probs = sorted_probs.clone()\n",
        "            filtered_probs[sorted_indices_to_remove] = 0\n",
        "            filtered_probs = filtered_probs / filtered_probs.sum()\n",
        "\n",
        "            next_token_idx = sorted_indices[torch.multinomial(filtered_probs, num_samples=1)].item()\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Estrategia no reconocida: {strategy}\")\n",
        "\n",
        "        if next_token_idx == eos_idx:\n",
        "            break\n",
        "\n",
        "        word = idx2word_target.get(next_token_idx, '<unk>')\n",
        "        if word not in ['<pad>', '<sos>', '<eos>', '<unk>']:\n",
        "            translation.append(word)\n",
        "\n",
        "        target_seq = torch.tensor([[next_token_idx]], dtype=torch.int64).to(device)\n",
        "\n",
        "    model.train()\n",
        "    return ' '.join(translation)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Generación de Ejemplos con Diferentes Estrategias de Decodificación\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "test_sentences = [\n",
        "    \"Tom is here.\",\n",
        "    \"Go home.\",\n",
        "    \"I am Tom.\",\n",
        "    \"Come here.\",\n",
        "    \"Be quiet.\"\n",
        "]\n",
        "\n",
        "for i, input_sentence in enumerate(test_sentences):\n",
        "    input_tensor = prepare_input_translation(input_sentence, word2idx_input, max_input_len)\n",
        "\n",
        "    print(f\"\\n[{i+1}] INPUT (EN): {input_sentence}\")\n",
        "\n",
        "    # Greedy Decoding\n",
        "    greedy_translation = translate_sentence(\n",
        "        input_tensor, model, idx2word_target, word2idx_target, max_target_len, strategy='greedy')\n",
        "    print(f\"    -> Greedy:           {greedy_translation}\")\n",
        "\n",
        "    # Muestreo Aleatorio (con temperatura)\n",
        "    sample_translation = translate_sentence(\n",
        "        input_tensor, model, idx2word_target, word2idx_target, max_target_len,\n",
        "        strategy='sample', temperature=0.8)\n",
        "    print(f\"    -> Sample (T=0.8):   {sample_translation}\")\n",
        "\n",
        "    # Top-k Sampling\n",
        "    topk_translation = translate_sentence(\n",
        "        input_tensor, model, idx2word_target, word2idx_target, max_target_len,\n",
        "        strategy='top_k', k=50)\n",
        "    print(f\"    -> Top-k (k=50):     {topk_translation}\")\n",
        "\n",
        "    # Top-p (Nucleus) Sampling\n",
        "    topp_translation = translate_sentence(\n",
        "        input_tensor, model, idx2word_target, word2idx_target, max_target_len,\n",
        "        strategy='top_p', p=0.9)\n",
        "    print(f\"    -> Top-p (p=0.9):    {topp_translation}\")\n",
        "\n",
        "    # Beam Search\n",
        "    beam_translation = translate_sentence(\n",
        "        input_tensor, model, idx2word_target, word2idx_target, max_target_len,\n",
        "        strategy='beam_search', beam_width=5)\n",
        "    print(f\"    -> Beam Search (w=5): {beam_translation}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}